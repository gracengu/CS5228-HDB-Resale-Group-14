{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/input/preprocessing-merge/\")\n",
    "sys.path.append(\"/kaggle/input/preprocessing-train-test/\")\n",
    "sys.path.append(\"/kaggle/input/clustering/\")\n",
    "\n",
    "from preprocessing_merge import *\n",
    "from preprocessing_train_test import *\n",
    "from clustering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('display.max_columns', 100)  # 设置显示100列\n",
    "pd.set_option('display.max_rows', 100)   # 设置显示100行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = \"/kaggle/input/preprocessed-data/merge_auxiliary_data_train.csv\"\n",
    "file_path_test = \"/kaggle/input/preprocessed-data/merge_auxiliary_data_test.csv\"\n",
    "file_path_clustering_train = \"/kaggle/input/preprocessed-data/pca_clustering_training.csv\"\n",
    "file_path_clustering_test = \"/kaggle/input/preprocessed-data/pca_clustering_testing.csv\"\n",
    "\n",
    "train_gdf = pd.read_csv(file_path_train)\n",
    "test_gdf = pd.read_csv(file_path_test)\n",
    "\n",
    "print(\"train dataset shape: \", train_gdf.shape)\n",
    "print(\"test dataset shape: \",test_gdf.shape)\n",
    "\n",
    "pca_cluster_df_training = pd.read_csv(file_path_clustering_train)\n",
    "pca_cluster_df_testing = pd.read_csv(file_path_clustering_test)\n",
    "\n",
    "train_gdf_updated = pd.concat([train_gdf, pca_cluster_df_training.reset_index(drop=True)], axis=1)\n",
    "test_gdf_updated = pd.concat([test_gdf, pca_cluster_df_testing.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"train dataset shape: \", train_gdf_updated.shape)\n",
    "print(\"test dataset shape: \",test_gdf_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best XGBoost model on a single 70:30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_gdf_updated.loc[:, ~train_gdf_updated.columns.isin([\"resale_price\"])], train_gdf_updated[\"resale_price\"], test_size=0.3, random_state=42)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "train_df, test_df = preprocess_train_test(train, test)\n",
    "x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "x_valid, y_valid = test_df.drop(columns=[\"resale_price\"], errors='ignore'), test_df[\"resale_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dataset: DataFrame):\n",
    "    # Visualizing the correlations between numerical variables\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(abs(dataset.corr()), cmap=\"YlGnBu\")\n",
    "    plt.title(\"Correlations Between Variables\", size=15)\n",
    "    plt.show()\n",
    "\n",
    "float_cols = list(train_df.select_dtypes(\"float\").columns)\n",
    "int_cols = list(train_df.select_dtypes(\"int\").columns)\n",
    "total_cols = float_cols + int_cols\n",
    "total_cols_filter = [i for i in total_cols if \"id\" not in i and i not in [\"latitude\", \"longitude\"]]\n",
    "\n",
    "visualize(train_df[total_cols_filter])\n",
    "\n",
    "a = calculateTopCorrelation(train_df[total_cols_filter])[1:16]\n",
    "labels = a['Columns']\n",
    "values = a['Pearson-score']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels, values)\n",
    "\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Pearson-score')\n",
    "ax.set_title('Correlation Analysis')\n",
    "ax.set_xticklabels(labels, fontsize=10, rotation=90)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_preprocess = ['floor_area_sqm', 'distance_to_mrt', 'nearest_mrt_counts',\n",
    "       'mrt_lrt_links', 'mrt_interchange_flag', 'mrt_interchange_count',\n",
    "       'population_count', 'adult_count', 'children_count',\n",
    "       'senior_citizen_count', 'teenager_count', 'young_adult_count',\n",
    "       'female_count', 'male_count', 'male_female_ratio',\n",
    "       'adult_children_ratio', 'nearest_dist_commerical',\n",
    "       'inRangeCount_commerical', 'nearest_dist_market', 'inRangeCount_market',\n",
    "       'MinPrimaryDist', 'nearPrimaryCount', 'MinSecDist', 'nearSecondCount',\n",
    "       'MinShopDist', 'nearShopCount', 'principal component 1',\n",
    "       'principal component 2', 'DBSCAN_cluster', 'resale_price',\n",
    "       'rebased_month', 'remaining_lease', '1-room', '2-room', '3-room',\n",
    "       '4-room', '5-room', 'executive', 'multi-generation', 'flat_type_price',\n",
    "       'flat_type_psm', 'flat_type_number', 'storey_range_start',\n",
    "       'storey_range_processed', 'storey_range_price',\n",
    "       'storey_range_price_psm', 'flat_model_price', 'flat_model_psm',\n",
    "       'grid_price', 'grid_price_psm', 'subzone_price', 'subzone_price_psm',\n",
    "       'planning_area_price', 'planning_area_price_psm', 'region_price',\n",
    "       'region_price_psm', 'mrt_type_price', 'mrt_codes_price',\n",
    "       'distance_to_mrt_bins_price', 'male_female_ratio_bins_price',\n",
    "       'adult_children_ratio_bins_price', 'population_bins_price',\n",
    "       'type_commerical_price']\n",
    "\n",
    "print([i for i in columns_preprocess if \"price\" in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best XGBoost model on a single 70:30 splitxw\n",
    "# Here we use a train_gdf dataset instead of train_gdf_updated as above as we tried the above updated dataset and did not yield good performance.\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_gdf.loc[:, ~train_gdf.columns.isin([\"resale_price\"])], train_gdf[\"resale_price\"], test_size=0.3, random_state=42)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "train_df, test_df = preprocess_train_test(train, test)\n",
    "x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "x_valid, y_valid = test_df.drop(columns=[\"resale_price\"], errors='ignore'), test_df[\"resale_price\"]\n",
    "\n",
    "regressor = xgb.XGBRegressor(n_estimators=2000, max_depth=6, min_child_weight=1.3, reg_alpha=1.6, reg_lambda=1.5, subsample=1, n_jobs=3).fit(x_train, y_train)\n",
    "y_train_predict = regressor.predict(x_train)\n",
    "y_valid_predict = regressor.predict(x_valid)\n",
    "\n",
    "print(mean_absolute_error(y_train, y_train_predict))\n",
    "print(mean_absolute_error(y_valid, y_valid_predict))\n",
    "print(mean_squared_error(y_train, y_train_predict))\n",
    "print(mean_squared_error(y_valid, y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Random Forest model on a single 70:30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_gdf.loc[:, ~train_gdf.columns.isin([\"resale_price\"])], train_gdf[\"resale_price\"], test_size=0.3, random_state=42)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "train_df, test_df = preprocess_train_test(train, test)\n",
    "x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "x_valid, y_valid = test_df.drop(columns=[\"resale_price\"], errors='ignore'), test_df[\"resale_price\"]\n",
    "\n",
    "regressor_rf = RandomForestRegressor(n_estimators=500, max_depth=24, max_features=0.6, min_samples_split=0.00001, min_samples_leaf=0.00001, n_jobs=3).fit(x_train, y_train)\n",
    "y_train_predict_rf = regressor_rf.predict(x_train)\n",
    "y_valid_predict_rf= regressor_rf.predict(x_valid)\n",
    "\n",
    "print(mean_absolute_error(y_train, y_train_predict_rf))\n",
    "print(mean_absolute_error(y_valid, y_valid_predict_rf))\n",
    "print(mean_squared_error(y_train, y_train_predict_rf))\n",
    "print(mean_squared_error(y_valid, y_valid_predict_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Ensemble Prediction\n",
    "y_train_final = np.mean([y_train_predict, y_train_predict_rf], axis=0)\n",
    "y_valid_final = np.mean([y_valid_predict, y_valid_predict_rf], axis=0)\n",
    "\n",
    "print(mean_absolute_error(y_train, y_train_final))\n",
    "print(mean_absolute_error(y_valid, y_valid_final))\n",
    "print(mean_squared_error(y_train, y_train_final))\n",
    "print(mean_squared_error(y_valid, y_valid_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(regressor, open(\"/kaggle/working/regressor_xgboost.pkl\", \"wb\"))\n",
    "# pickle.dump(regressor_rf, open(\"/kaggle/working/regressor_rf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full training and testing\n",
    "\n",
    "Implement the above code on full training and testing data i.e. Train on full training dataset, and generate predictions for test dataset for kaggle submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdf = pd.read_csv('/kaggle/input/preprocessed-data/merge_auxiliary_data_train.csv')\n",
    "test_gdf = pd.read_csv('/kaggle/input/preprocessed-data/merge_auxiliary_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the model was previously saved in /kaggle/working as mentioned above \n",
    "# -> as session has stopped and paused, reuploaded the files in /kaggle/input/final-models\n",
    "regressor_xgboost = pickle.load(open(\"/kaggle/input/final-models/regressor_xgboost_updated.pkl\", \"rb\"))\n",
    "regressor_randomforest = pickle.load(open(\"/kaggle/input/final-models/regressor_rf_updated_v3.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit models on full Train dataframe\n",
    "train_df_all, test_df_all = preprocess_train_test(train_gdf, test_gdf)\n",
    "x_train = train_df_all.drop(columns=[\"resale_price\"], errors='ignore')\n",
    "# print(\"Fitting xgboost_model\")\n",
    "# regressor_xgboost.fit(x_train, train_df_all['resale_price'])\n",
    "# print(\"Fitting random forest model\")\n",
    "# regressor_randomforest.fit(x_train, train_df_all['resale_price'])\n",
    "\n",
    "# Performing prediction\n",
    "y_train_predict = regressor_xgboost.predict(x_train)\n",
    "y_train_predict2 = regressor_randomforest.predict(x_train)\n",
    "res_train = pd.DataFrame(np.mean([y_train_predict, y_train_predict2], axis=0), columns=['Predicted'])\n",
    "print(mean_absolute_error(train_df_all['resale_price'], res_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save updated models\n",
    "# pickle.dump(regressor_backup, open(\"regressor_xgboost_updated.pkl\", \"wb\"))\n",
    "# pickle.dump(regressor_v2_backup, open(\"regressor_rf_updated.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for full test dataframe\n",
    "x_test = test_df_all.copy()\n",
    "y_test_predict = regressor_xgboost.predict(x_test)\n",
    "y_test_predict2 = regressor_randomforest.predict(x_test)\n",
    "res_test = pd.DataFrame(np.mean([y_test_predict, y_test_predict2], axis=0), columns=['Predicted'])\n",
    "res2 = res_test.reset_index().rename(columns={'index':'Id'})\n",
    "res2.to_csv('res2.csv', index=False) # This is the dataframe, submitted to kaggle that generate the best scores\n",
    "display(res2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Plot\n",
    "\n",
    "These are the feature importance plot for the latest best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the model was previously saved in /kaggle/working as mentioned above \n",
    "# -> as session has stopped and paused, reuploaded the files in /kaggle/input/final-models\n",
    "regressor_xgboost = pickle.load(open(\"/kaggle/input/final-models/regressor_xgboost_updated.pkl\", \"rb\"))\n",
    "regressor_randomforest = pickle.load(open(\"/kaggle/input/final-models/regressor_rf_updated_v3.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Feature Importance Plot\n",
    "feature_impotance_df = pd.DataFrame({\"Features\":regressor_xgboost.get_booster().feature_names, \"Feature Importance\":regressor_xgboost.feature_importances_})\n",
    "feature_impotance_df = feature_impotance_df.set_index(\"Features\", inplace=False)\n",
    "feature_impotance_df_filter = feature_impotance_df.sort_values(\"Feature Importance\", ascending=False)[:10]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_pos = np.arange(len(feature_impotance_df_filter.index))\n",
    "ax.barh(y_pos, feature_impotance_df_filter[\"Feature Importance\"], align='center')\n",
    "ax.set_yticks(y_pos, labels=feature_impotance_df_filter.index)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('XGBRegressor Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impotance_df = pd.DataFrame({\"Features\":regressor_randomforest.feature_names_in_, \"Feature Importance\":regressor_randomforest.feature_importances_})\n",
    "feature_impotance_df = feature_impotance_df.set_index(\"Features\", inplace=False)\n",
    "feature_impotance_df_filter = feature_impotance_df.sort_values(\"Feature Importance\", ascending=False)[:15]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_pos = np.arange(len(feature_impotance_df_filter.index))\n",
    "ax.barh(y_pos, feature_impotance_df_filter[\"Feature Importance\"], align='center')\n",
    "ax.set_yticks(y_pos, labels=feature_impotance_df_filter.index)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Random Forest Regressor Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
