{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from preprocessing_train_test import *\n",
    "\n",
    "from scipy.stats import skew \n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "pd.set_option('display.max_columns', 100)  # 设置显示100列\n",
    "pd.set_option('display.max_rows', 100)   # 设置显示100行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = \"../data/backup/merge_auxiliary_data_train.csv\"\n",
    "file_path_test = \"../data/backup/merge_auxiliary_data_test.csv\"\n",
    "\n",
    "train_gdf = pd.read_csv(file_path_train)\n",
    "test_gdf = pd.read_csv(file_path_test)\n",
    "\n",
    "print(\"train dataset shape: \", train_gdf.shape)\n",
    "print(\"test dataset shape: \",test_gdf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training with max_depth: 10, min_child_weight: 1, reg_alpha: 0.464, reg_lambda: 0.8571\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7012.332748009691, 7018.390488505503, 7073.70992372483, 7039.810206739005, 7026.969783400063] [11954.903251394084, 11961.14115118034, 12012.586101370867, 11922.27561574074, 12025.305884152856] [89330932.16716915, 89399592.45041788, 90924765.08313645, 90356313.03031243, 89730226.58213118] [293904289.880039, 289338830.66374123, 294794189.6729655, 288098081.8241463, 297062677.1516727]\n",
      "start training with max_depth: 10, min_child_weight: 1, reg_alpha: 0.464, reg_lambda: 1\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7054.256138670036, 7084.038155364204, 6933.665445863486, 7065.468501600357, 7048.991124889575] [11912.326448058053, 11937.108235513393, 12041.252286970082, 11975.573601630358, 12005.722252546804] [90476268.78505792, 91477277.45996505, 87259271.30644193, 90932851.202957, 90735676.96892817] [290596531.6451196, 290480199.7133786, 297487569.0646303, 290758816.2587549, 295378292.5152268]\n",
      "start training with max_depth: 10, min_child_weight: 1, reg_alpha: 1, reg_lambda: 0.8571\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[6979.230747112149, 6955.103602975796, 6977.114197103475, 7025.118564633847, 6941.852377152303] [11965.851573050091, 11937.575307716612, 11942.704726342998, 11988.368258934983, 11969.076444365908] [88069926.13427357, 87909205.6579085, 88366398.63814484, 89717070.74190149, 87503202.03262144] [291830378.45742047, 293040683.3874701, 290832764.47158706, 293830956.69569504, 293474663.7580221]\n",
      "start training with max_depth: 10, min_child_weight: 1, reg_alpha: 1, reg_lambda: 1\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[6996.743170449966, 7069.213680140988, 7052.101565411457, 7031.754424760706, 7031.281299431022] [11969.873409908565, 11922.642928393212, 11885.781531608296, 11982.2910655144, 12024.24935588577] [89106833.7792522, 91069287.71911965, 90766345.38392767, 89928369.68881348, 90062907.54239689] [292419390.7230299, 285184329.2191663, 291794396.29127437, 294851995.7621258, 296755694.1535506]\n",
      "start training with max_depth: 10, min_child_weight: 1.5, reg_alpha: 0.464, reg_lambda: 0.8571\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7136.889717321761, 7103.792504732582, 7076.180780178665, 7027.8678354659505, 7029.492143717953] [12001.314534205012, 12011.969265620477, 11964.319195897177, 11997.219797508355, 12006.979314003398] [92823767.7611615, 91472672.97042723, 90768034.3742373, 89726267.11372738, 89483504.11324722] [293147234.7128107, 296058264.273903, 289997930.74260795, 296074752.22331554, 297123179.4667935]\n",
      "start training with max_depth: 10, min_child_weight: 1.5, reg_alpha: 0.464, reg_lambda: 1\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7148.906845976106, 7097.235763847944, 7159.241847817753, 7205.758135415728, 7127.5882087194905] [11950.56219054802, 11970.851035064548, 11946.633253272443, 11964.41012436339, 12048.694792183844] [92899777.60338755, 91668304.78679872, 93171950.11127816, 94406374.11786765, 92476689.20433617] [292058086.9508248, 290549929.77369195, 286990478.64995897, 294336241.216684, 296662144.8404555]\n",
      "start training with max_depth: 10, min_child_weight: 1.5, reg_alpha: 1, reg_lambda: 0.8571\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7109.85965920059, 7141.823652718212, 7079.513893964035, 7022.228813825615, 7215.457729213436] [11978.835860152205, 11910.89676564979, 11982.070423009607, 12032.717638329726, 11999.629045942342] [91773188.51505917, 92770251.89108157, 91030461.84561776, 89688869.77256669, 94799684.20251086] [292698608.88293076, 287986490.7312143, 293552809.3469811, 299395332.4468004, 290982572.9666444]\n",
      "start training with max_depth: 10, min_child_weight: 1.5, reg_alpha: 1, reg_lambda: 1\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7136.310444012531, 7062.451367700508, 7163.671424384317, 7266.933992197347, 7149.818788037103] [11985.017768026106, 11974.212856900427, 11953.050139476642, 11874.566995624073, 12037.828102348401] [92550955.88923456, 90494549.84103625, 93373263.28149058, 96282806.23058163, 92827556.84305625] [296457550.7101742, 294348388.94427043, 289266992.29108167, 283894185.8338401, 293804379.46691954]\n",
      "start training with max_depth: 10, min_child_weight: 1.7817, reg_alpha: 0.464, reg_lambda: 0.8571\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n",
      "[7068.016573117681, 7127.49864337881, 7081.911686560678, 7085.99585513545, 7104.994165737053] [11962.489581777956, 11998.518370983502, 12047.036811767843, 11934.638341360993, 11999.752065299059] [90842841.65017399, 92190987.94362132, 91132171.68575752, 91295300.85863167, 91663053.47261022] [292130555.1806323, 298473647.32428056, 293677584.9724016, 291280945.47709394, 292373585.2395833]\n",
      "start training with max_depth: 10, min_child_weight: 1.7817, reg_alpha: 0.464, reg_lambda: 1\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "fold 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8l/73hx55c95lb5pt533_s7mbvc0000gn/T/ipykernel_16561/180909201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mx_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0my_train_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1680\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1681\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_depths = [6]\n",
    "min_child_weights = [1, 1.5, 1.7817, 2]\n",
    "reg_alphas = [0.4640, 1] \n",
    "reg_lambdas = [0.8571, 1]\n",
    "\n",
    "# max_depths = [10]\n",
    "# min_child_weights = [1.5, 1.7817]\n",
    "# reg_alphas = [0.4640, 1] \n",
    "# reg_lambdas = [0.8571, 1]\n",
    "\n",
    "scores_xgb = {}\n",
    "for max_depth in max_depths:\n",
    "    for min_child_weight in min_child_weights:\n",
    "        for reg_alpha in reg_alphas:\n",
    "            for reg_lambda in reg_lambdas:\n",
    "                print('start training with max_depth: {}, min_child_weight: {}, reg_alpha: {}, reg_lambda: {}'.format(max_depth, min_child_weight, reg_alpha, reg_lambda))\n",
    "                mae_test, mae_valid, mse_test, mse_valid = [], [], [], []\n",
    "                split = KFold(n_splits=5, shuffle=True).split(train_gdf)\n",
    "                for idx, (train_idx, validate_idx) in enumerate(split):\n",
    "                    print('fold {}'.format(idx + 1))\n",
    "                    train_df, validate_df = preprocess_train_test(train_gdf.iloc[list(train_idx)], train_gdf.iloc[list(validate_idx)])\n",
    "\n",
    "                    x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "                    x_valid, y_valid = validate_df.drop(columns=[\"resale_price\"], errors='ignore'), validate_df[\"resale_price\"]\n",
    "\n",
    "                    features = pd.concat([x_train, x_valid]).reset_index(drop=True)\n",
    "                    overfit = []\n",
    "                    for i in features.columns:\n",
    "                        counts = features[i].value_counts()\n",
    "                        zeros = counts.iloc[0]\n",
    "                        if zeros / len(features) * 100 > 99.94:\n",
    "                            overfit.append(i)\n",
    "\n",
    "                    overfit = list(overfit)\n",
    "                    final_features = features.drop(overfit, axis=1)\n",
    "\n",
    "                    x_train = final_features.iloc[:len(x_train), :]\n",
    "                    x_valid = final_features.iloc[len(x_train):, :]\n",
    "\n",
    "                    regressor = xgb.XGBRegressor(n_estimators=350, max_depth=max_depth, min_child_weight=min_child_weight, reg_alpha=reg_alpha, reg_lambda=reg_lambda, n_jobs=3).fit(x_train, y_train)\n",
    "                    \n",
    "                    y_train_predict = regressor.predict(x_train)\n",
    "                    y_valid_predict = regressor.predict(x_valid)\n",
    "\n",
    "                    mae_test.append(mean_absolute_error(y_train, y_train_predict))\n",
    "                    mae_valid.append(mean_absolute_error(y_valid, y_valid_predict))\n",
    "                    mse_test.append(mean_squared_error(y_train, y_train_predict))\n",
    "                    mse_valid.append(mean_squared_error(y_valid, y_valid_predict))\n",
    "\n",
    "                print(mae_test, mae_valid, mse_test, mse_valid)\n",
    "                scores_xgb[(max_depth, min_child_weight, reg_alpha, reg_lambda)] = (mae_test, mae_valid, mse_test, mse_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Run best random forest hyperparam on full dataset'''\n",
    "# test_gdf.drop(columns=[\"opening_year\"], inplace=True, errors='ignore')\n",
    "# train_df, test_df = preprocess_train_test(train_gdf, test_gdf)\n",
    "\n",
    "# x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "\n",
    "# # x_valid, y_valid = validate_df.drop(columns=drop_columns, errors='ignore'), validate_df[\"resale_price\"]\n",
    "\n",
    "# # regressor = xgb.XGBRegressor(n_estimators=10, max_depth=10, min_child_weight=1.5, reg_alpha=1, reg_lambda=0.8571, n_jobs=3).fit(x_train, y_train)\n",
    "# regressor = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "#                              learning_rate=0.05, max_depth=6, \n",
    "#                              min_child_weight=1.5, n_estimators=500,\n",
    "#                              reg_alpha=1, reg_lambda=0.8571,\n",
    "#                              subsample=0.5213, nthread = -1).fit(x_train, y_train)\n",
    "\n",
    "# y_train_predict = regressor.predict(x_train)\n",
    "# # y_valid_predict = regressor.predict(x_valid)\n",
    "\n",
    "# print(mean_absolute_error(y_train, y_train_predict))\n",
    "# # mae_valid.append(mean_absolute_error(y_valid, y_valid_predict))\n",
    "# print(mean_squared_error(y_train, y_train_predict))\n",
    "# # mse_valid.append(mean_squared_error(y_valid, y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mae_key, min_mae = None, 100000\n",
    "min_mse_key, min_mse = None, 10000000000000\n",
    "for key, (_, mae_valid, _, mse_valid) in scores_xgb.items():\n",
    "    if np.mean(mae_valid) < min_mae:\n",
    "        min_mae_key = key\n",
    "        min_mae = np.mean(mae_valid)\n",
    "    if np.mean(mse_valid) < min_mse:\n",
    "        min_mse_key = key\n",
    "        min_mse = np.mean(mse_valid)\n",
    "print(min_mae_key, min_mae)\n",
    "print(min_mse_key, min_mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training with num_leave: 4, max_bin: 100, bagging_fraction: 100, bagging_freq: 0.75\n",
      "fold 1\n",
      "fold 2\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=1.0 will be ignored. Current value: bagging_fraction=0.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8l/73hx55c95lb5pt533_s7mbvc0000gn/T/ipykernel_8684/3767717149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                 \u001b[0mfeature_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                 feature_fraction_seed=7).fit(x_train, y_train)                    \n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0my_train_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0my_valid_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m                              % (self._n_features, n_features))\n\u001b[1;32m    687\u001b[0m         return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n\u001b[0;32m--> 688\u001b[0;31m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m         return predictor.predict(data, start_iteration, num_iteration,\n\u001b[1;32m   2958\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m                                  data_has_header, is_reshape)\n\u001b[0m\u001b[1;32m   2960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pred_for_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pred_for_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     def __create_sparse_native(self, cs, out_shape, out_ptr_indptr, out_ptr_indices, out_ptr_data,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envname/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36minner_predict\u001b[0;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_parameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_num_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[1;32m    660\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_preds\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mout_num_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length for predict results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_leaves = [4, 6]\n",
    "max_bins = [100, 200]\n",
    "bagging_fractions = [0.7, 0.8] \n",
    "bagging_freqs = [3, 10]\n",
    "\n",
    "# num_leaves = [4]\n",
    "# max_bins = [100, 400]\n",
    "# bagging_fractions = [0.75] \n",
    "# bagging_freqs = [3]\n",
    "\n",
    "scores_lightgbm = {}\n",
    "for num_leave in num_leaves:\n",
    "    for max_bin in max_bins:\n",
    "        for bagging_fraction in bagging_fractions:\n",
    "            for bagging_freq in bagging_freqs:\n",
    "                print('start training with num_leave: {}, max_bin: {}, bagging_fraction: {}, bagging_freq: {}'.format(num_leave, max_bin, max_bin, bagging_fraction))\n",
    "                mae_test, mae_valid, mse_test, mse_valid = [], [], [], []\n",
    "                split = KFold(n_splits=5, shuffle=True).split(train_gdf)\n",
    "                for idx, (train_idx, validate_idx) in enumerate(split):\n",
    "                    print('fold {}'.format(idx + 1))\n",
    "                    train_df, validate_df = preprocess_train_test(train_gdf.iloc[list(train_idx)], train_gdf.iloc[list(validate_idx)])\n",
    "\n",
    "                    x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "                    x_valid, y_valid = validate_df.drop(columns=[\"resale_price\"], errors='ignore'), validate_df[\"resale_price\"]\n",
    "\n",
    "                    features = pd.concat([x_train, x_valid]).reset_index(drop=True)\n",
    "                    overfit = []\n",
    "                    for i in features.columns:\n",
    "                        counts = features[i].value_counts()\n",
    "                        zeros = counts.iloc[0]\n",
    "                        if zeros / len(features) * 100 > 99.94:\n",
    "                            overfit.append(i)\n",
    "\n",
    "                    overfit = list(overfit)\n",
    "                    final_features = features.drop(overfit, axis=1)\n",
    "\n",
    "                    x_train = final_features.iloc[:len(x_train), :]\n",
    "                    x_valid = final_features.iloc[len(x_train):, :]\n",
    "\n",
    "                    regressor = LGBMRegressor(objective='regression', \n",
    "                                                n_estimators=300,\n",
    "                                                num_leaves=num_leave,\n",
    "                                                max_bin=max_bin, \n",
    "                                                learning_rate=0.01, \n",
    "                                                bagging_fraction=bagging_fraction,\n",
    "                                                bagging_freq=bagging_freq, \n",
    "                                                bagging_seed=7,\n",
    "                                                feature_fraction=0.2,\n",
    "                                                feature_fraction_seed=7).fit(x_train, y_train)                    \n",
    "                    y_train_predict = regressor.predict(x_train)\n",
    "                    y_valid_predict = regressor.predict(x_valid)\n",
    "\n",
    "                    mae_test.append(mean_absolute_error(y_train, y_train_predict))\n",
    "                    mae_valid.append(mean_absolute_error(y_valid, y_valid_predict))\n",
    "                    mse_test.append(mean_squared_error(y_train, y_train_predict))\n",
    "                    mse_valid.append(mean_squared_error(y_valid, y_valid_predict))\n",
    "\n",
    "                print(mae_test, mae_valid, mse_test, mse_valid)\n",
    "                scores_lightgbm[(max_depth, min_child_weight, reg_alpha, reg_lambda)] = (mae_test, mae_valid, mse_test, mse_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Run best random forest hyperparam on full dataset'''\n",
    "# test_gdf.drop(columns=[\"opening_year\"], inplace=True, errors='ignore')\n",
    "# train_df, test_df = preprocess_train_test(train_gdf, test_gdf)\n",
    "\n",
    "# x_train, y_train = train_df.drop(columns=[\"resale_price\"], errors='ignore'), train_df[\"resale_price\"]\n",
    "\n",
    "# # x_valid, y_valid = validate_df.drop(columns=drop_columns, errors='ignore'), validate_df[\"resale_price\"]\n",
    "\n",
    "# regressor = LGBMRegressor(objective='regression', \n",
    "#                             n_estimators=500,\n",
    "#                             num_leaves=num_leave,\n",
    "#                             max_bin=max_bin, \n",
    "#                             learning_rate=0.01, \n",
    "#                             bagging_fraction=bagging_fraction,\n",
    "#                             bagging_freq=bagging_freq, \n",
    "#                             bagging_seed=7,\n",
    "#                             feature_fraction=0.2,\n",
    "#                             feature_fraction_seed=7).fit(x_train, y_train)\n",
    "# y_train_predict = regressor.predict(x_train)\n",
    "# # y_valid_predict = regressor.predict(x_valid)\n",
    "\n",
    "# print(mean_absolute_error(y_train, y_train_predict))\n",
    "# # mae_valid.append(mean_absolute_error(y_valid, y_valid_predict))\n",
    "# print(mean_squared_error(y_train, y_train_predict))\n",
    "# # mse_valid.append(mean_squared_error(y_valid, y_valid_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mae_key, min_mae = None, 100000\n",
    "min_mse_key, min_mse = None, 10000000000000\n",
    "for key, (_, mae_valid, _, mse_valid) in scores_lightgbm.items():\n",
    "    if np.mean(mae_valid) < min_mae:\n",
    "        min_mae_key = key\n",
    "        min_mae = np.mean(mae_valid)\n",
    "    if np.mean(mse_valid) < min_mse:\n",
    "        min_mse_key = key\n",
    "        min_mse = np.mean(mse_valid)\n",
    "print(min_mae_key, min_mae)\n",
    "print(min_mse_key, min_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
